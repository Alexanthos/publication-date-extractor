{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "t2e8ZxUZ-gUp"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Cours\\Master\\NLP in industry\\publication-date-extractor\\env\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from jinja2 import Template\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "QjSwT9TC-jGF"
   },
   "outputs": [],
   "source": [
    "prompt_template = Template(\n",
    "    \"\"\"\n",
    "    You are an expert in structured data extraction.\n",
    "    Given a document in French, extract and output only the **publication date** of the document in the format DD/MM/YYYY. Do not include any additional text or context — just the date.\n",
    "\n",
    "    Document: \"{{ document }}\"\n",
    "    Publication date:\n",
    "    \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uXDknDrcXWTB",
    "outputId": "201e9c8d-f89c-4868-eecf-64d9695cac50"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: bitsandbytes in d:\\cours\\master\\nlp in industry\\publication-date-extractor\\env\\lib\\site-packages (0.44.1)\n",
      "Requirement already satisfied: torch in d:\\cours\\master\\nlp in industry\\publication-date-extractor\\env\\lib\\site-packages (from bitsandbytes) (2.5.1+cu118)\n",
      "Requirement already satisfied: numpy in d:\\cours\\master\\nlp in industry\\publication-date-extractor\\env\\lib\\site-packages (from bitsandbytes) (2.1.3)\n",
      "Requirement already satisfied: filelock in d:\\cours\\master\\nlp in industry\\publication-date-extractor\\env\\lib\\site-packages (from torch->bitsandbytes) (3.16.1)\n",
      "Requirement already satisfied: jinja2 in d:\\cours\\master\\nlp in industry\\publication-date-extractor\\env\\lib\\site-packages (from torch->bitsandbytes) (3.1.4)\n",
      "Requirement already satisfied: fsspec in d:\\cours\\master\\nlp in industry\\publication-date-extractor\\env\\lib\\site-packages (from torch->bitsandbytes) (2024.9.0)\n",
      "Requirement already satisfied: networkx in d:\\cours\\master\\nlp in industry\\publication-date-extractor\\env\\lib\\site-packages (from torch->bitsandbytes) (3.4.2)\n",
      "Requirement already satisfied: sympy==1.13.1 in d:\\cours\\master\\nlp in industry\\publication-date-extractor\\env\\lib\\site-packages (from torch->bitsandbytes) (1.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in d:\\cours\\master\\nlp in industry\\publication-date-extractor\\env\\lib\\site-packages (from torch->bitsandbytes) (4.12.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in d:\\cours\\master\\nlp in industry\\publication-date-extractor\\env\\lib\\site-packages (from sympy==1.13.1->torch->bitsandbytes) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in d:\\cours\\master\\nlp in industry\\publication-date-extractor\\env\\lib\\site-packages (from jinja2->torch->bitsandbytes) (3.0.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.0.1 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install -U bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1s__Ii6TpFsi",
    "outputId": "2590e9d3-839b-4b85-d7b1-07bea8446f3b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Cours\\Master\\NLP in industry\\publication-date-extractor\\env\\lib\\site-packages\\huggingface_hub\\file_download.py:139: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Alex\\.cache\\huggingface\\hub\\models--unsloth--Llama-3.2-3B. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
      "`low_cpu_mem_usage` was None, now default to True since model is quantized.\n"
     ]
    }
   ],
   "source": [
    "unsloth_checkpoint = \"unsloth/Llama-3.2-3B\"\n",
    "model = AutoModelForCausalLM.from_pretrained(unsloth_checkpoint,\n",
    "                                             load_in_8bit=True)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(unsloth_checkpoint)\n",
    "\n",
    "def llm_complete(prompt, max_tokens=2048, device = 'cuda', temperature=0.5):\n",
    "    # Fill it\n",
    "    # model.to(device)\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    inputs.to(device)\n",
    "    outputs = model.generate(**inputs, max_new_tokens=max_tokens, temperature = temperature, pad_token_id=tokenizer.eos_token_id)\n",
    "    outputs_ans_only = outputs[:,len(inputs['input_ids'][0]):]\n",
    "    answer_only = tokenizer.batch_decode(outputs_ans_only, skip_special_tokens=True)\n",
    "    return answer_only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "zcDZcVeApkBa"
   },
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "#llm_complete(prompt, max_tokens = 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nr2nVc2JxBkD"
   },
   "source": [
    "TODO:\n",
    "- use regex to generate text chunks, then join them into one long context\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 434
    },
    "id": "8VF9rX3eseHT",
    "outputId": "c42ac67b-56c5-4d97-9597-f3d9f328a006"
   },
   "outputs": [],
   "source": [
    "df = pd.read_pickle('df_with_regex_chunks.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "1iqtIJbH0cMp"
   },
   "outputs": [],
   "source": [
    "def predict_date(document, temperature=0.5):\n",
    "    input = {'document': document}\n",
    "    prompt = prompt_template.render(**input)\n",
    "    output = llm_complete(prompt, max_tokens = 10, temperature=temperature)\n",
    "    date = re.findall(r\"\\d{2}/\\d{2}/\\d{4}\\b\", str(output))\n",
    "    if date: return date[0]\n",
    "    else: return str(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "viOyZC8C0cMs",
    "outputId": "4405e9ba-8a69-4334-d1dc-886adcd7a5ae"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "175    31/01/2023\n",
      "248    12/12/2022\n",
      "3      26/01/2023\n",
      "231    20/03/2024\n",
      "396    30/09/2020\n",
      "208    03/10/2022\n",
      "70     06/02/2023\n",
      "41     03/11/2022\n",
      "360          None\n",
      "417    25/03/2013\n",
      "Name: Gold published date, dtype: object\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Cours\\Master\\NLP in industry\\publication-date-extractor\\env\\lib\\site-packages\\transformers\\generation\\configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.5` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "175                                     08/02/2023\n",
      "248                                     12/12/2022\n",
      "3                                       26/01/2023\n",
      "231                                     06/03/2024\n",
      "396    [' 30 septembre 2020\\n    Expected output']\n",
      "208                              [' 2022-06-27\\n']\n",
      "70                 [' 2015-12-17\\n\\n    Document']\n",
      "41                                      04/09/2019\n",
      "360                                     26/11/2020\n",
      "417                                     25/03/2013\n",
      "Name: prediction, dtype: object\n"
     ]
    }
   ],
   "source": [
    "test = df.sample(10)\n",
    "print(test['Gold published date'])\n",
    "test['prediction'] = test.apply(lambda x: predict_date(str(x['regex_chunks'])+x['doc_id']), axis=1)\n",
    "print(test['prediction'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "9jEMoHJ20cMv"
   },
   "outputs": [],
   "source": [
    "df['prediction'] = df.apply(lambda x: predict_date(str(x['regex_chunks'])+x['doc_id']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 444
    },
    "id": "hF0ukIlS0cM0",
    "outputId": "0d04a537-1a27-4fec-8f22-58c411303661"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Gold published date</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>16/01/2023</td>\n",
       "      <td>[' * 16 January 2023\\n    Expected']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>25/01/2023</td>\n",
       "      <td>25/01/2023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>02/02/2023</td>\n",
       "      <td>02/02/2023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>26/01/2023</td>\n",
       "      <td>26/01/2023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>16/01/2023</td>\n",
       "      <td>16/01/2023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>02/04/2024</td>\n",
       "      <td>24/01/2024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>09/01/2024</td>\n",
       "      <td>[' - 2024-01-10\\n   ']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>22/11/2022</td>\n",
       "      <td>22/11/2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>21/12/2023</td>\n",
       "      <td>15/12/2023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>22/12/2023</td>\n",
       "      <td>22/12/2023</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>500 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Gold published date                            prediction\n",
       "0            16/01/2023  [' * 16 January 2023\\n    Expected']\n",
       "1            25/01/2023                            25/01/2023\n",
       "2            02/02/2023                            02/02/2023\n",
       "3            26/01/2023                            26/01/2023\n",
       "4            16/01/2023                            16/01/2023\n",
       "..                  ...                                   ...\n",
       "495          02/04/2024                            24/01/2024\n",
       "496          09/01/2024                [' - 2024-01-10\\n   ']\n",
       "497          22/11/2022                            22/11/2022\n",
       "498          21/12/2023                            15/12/2023\n",
       "499          22/12/2023                            22/12/2023\n",
       "\n",
       "[500 rows x 2 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[['Gold published date','prediction']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "gcrxRIDj0cM7"
   },
   "outputs": [],
   "source": [
    "df.to_pickle('huggingface_llama.pkl')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
